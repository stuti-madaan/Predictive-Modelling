---
title: 'STA380, Part2: Exercises 1'
author: "Nimish Amlathe, Hitesh Prabhu, Stuti Madaan"
date: "August 8, 2016"
output:
  word_document: default
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 11, fig.height = 7, echo = TRUE)
```

# Probability Practice

## Part A

**Here's a question a friend of mine was asked when he interviewed at Google.**

**Visitors to your website are asked to answer a single survey question before they get access to the content on the page. Among all of the users, there are two categories: Random Clicker (RC), and Truthful Clicker (TC). There are two possible answers to the survey: yes and no. Random clickers would click either one with equal probability. You are also giving the information that the expected fraction of random clickers is 0.3.**

**After a trial period, you get the following survey results: 65% said Yes and 35% said No.**

**What fraction of people who are truthful clickers answered yes?**

**Solution:**

$Prob(RC) = 0.3$

$Prob(TC)=1-0.3= 0.7$

$Prob(Yes|RC)= Prob(No|RC)=0.5$

$Prob(Yes) = 0.65$

$Prob(Yes|TC) =?$

We solved this problem by using the concept of Total Sum of Probilities which states that:

$Prob(Yes) = Prob(Yes|RC) * Prob(RC) + Prob(Yes|TC) * Prob(TC)$

$0.65 = 0.5 *0.3 + Prob(Yes|TC) * 0.7$

$0.65-0.15 = Prob(Yes|TC) *0.7$

$0.5 = Prob(Yes|TC) *0.7$

$Prob(Yes|TC) = 0.5/0.7 = 0.71$

We know that the probability of a Random clicker is 0.3. Also, we know that the probability of Yes/No given a Random Clicker is 0.5. From this, we can derive the overall probaility of a Yes/No from a Random Clicker = 0.5 * 0.3 = 0.15. 

Thus, the fraction of 'Yes'es from a Truthful speaker = 0.5/0.7 = `r 0.5/0.7`


## Part B

**Imagine a medical test for a disease with the following two attributes:**

* **The sensitivity is about 0.993. That is, if someone has the disease, there is a probability of 0.993 that they will test positive.**

* **The specificity is about 0.9999. This means that if someone doesn't have the disease, there is probability of 0.9999 that they will test negative.**

**In the general population, incidence of the disease is reasonably rare: about 0.0025% of all people have it (or 0.000025 as a decimal probability).**

**Suppose someone tests positive. What is the probability that they have the disease? In light of this calculation, do you envision any problems in implementing a universal testing policy for the disease?**

**Solution:**

$Prob(Positive|Disease) = 0.993$

$Prob(Negative|Does not have Disease)=0.9999$

$Prob(Disease)=0.000025$

$Prob(Disease|Postive)= (P(Positive|Disease) * P(Disease))/ (P(Positive|Disease)* P(Disease)+P(Positive|No Disease)* P(Does Not Have Disease))$

$= (0.993 * 0.000025)/(0.993 *0.000025 + (1-0.9999)* (1-0.000025))$

$= `r (0.993 * 0.000025)/(0.993* 0.000025 + (1-0.9999)* (1-0.000025))`$

If a test's result is positive, the probability that there is a disease is very less i.e.approximately 0.2. This implies that there are a lot of false positives from this test. If this is implemented as a universal testing policy, a lot of people will be falsely informed of having the disease when they don't. In medical world, this would be a blunder.

## Exploratory analysis: green buildings

#### Loading data and loading

```{r GreenBuildings loading data}
greenBuildings <- read.csv("files/greenbuildings.csv")

summary(greenBuildings[which(greenBuildings$green_rating == 0 
                            & greenBuildings$leasing_rate > 10) ,'Rent'], na.rm = T)
summary(greenBuildings[which(greenBuildings$green_rating == 1 
                            & greenBuildings$leasing_rate > 10) ,'Rent'], na.rm = T)
```

#### 1. Looking at the relationship between the variables *EnergyStar*, *LEED* and *green_rating*

The reason we decided to investigate this first is because 

EPA's ENERGY STAR identifies the nation's most energy-efficient commercial buildings and industrial plants. Through ENERGY STAR, EPA offers 1 â€“ 100 *ENERGY STAR* scores that rate buildings against their peers. To earn the ENERGY STAR, a fully operational facility must earn an ENERGY STAR score of 75 or higher, meaning that it performs in the top 25 percent of similar facilities nationwide for energy efficiency.


LEED is a green building rating system administered by the private non-profit U.S. Green Building Council.  LEED addresses several environmental attributes in addition to energy efficiency, such as materials, waste, and water. To earn LEED certification, a building does not always need to meet the rigorous energy performance level required to earn EPA's ENERGY STAR.


While LEED can help organizations achieve a wide range of sustainability goals, ENERGY STAR certification is the only way to ensure superior energy performance. For this reason, the two programs can work very well together.


LEED is frowned upon by many owners and investors, however, because it can be incredibly expensive to become certified. Many owners, developers and investors pass on LEED certification because the additional cost of commissioning, paperwork and professional fees seems daunting and unnecessary. In fact, LEED and Energy Star are complimentary to each other. Buildings may be both LEED certified and Energy Star rated, and LEED requires Energy Star as part of its EB (Existing Building) rating system.

```{r EnergyStar and LEED x-tabs, echo = F}
library(knitr)
library(xtable)

# Checking relationship between EnergyStar and LEED ratings with green_rating
green_EnergyStar_LEED <- xtabs(~green_rating 
                              + Energystar 
                              + LEED, data = greenBuildings)
ftable(green_EnergyStar_LEED) # print table 
```

There are only 47 buildings which are LEED certified amongst the 685 green buildings. The rest are Energy-star rated buildings. Now let's look at the median rents of these subgroups: 

```{r EnergyStar and LEED summmaries, echo = T}
# Checking medians of buildings which are amongst the categories in the x-tab above
summary(greenBuildings[which(greenBuildings$Energystar == 0 
                            & greenBuildings$LEED == 0 
                            & greenBuildings$leasing_rate > 10) ,'Rent'], na.rm = T)
summary(greenBuildings[which(greenBuildings$Energystar == 1 
                            & greenBuildings$LEED == 0 
                            & greenBuildings$leasing_rate > 10) ,'Rent'], na.rm = T)
summary(greenBuildings[which(greenBuildings$Energystar == 0 
                            & greenBuildings$LEED == 1 
                            & greenBuildings$leasing_rate > 10) ,'Rent'], na.rm = T)
summary(greenBuildings[which(greenBuildings$Energystar == 1 
                            & greenBuildings$LEED == 1 
                            & greenBuildings$leasing_rate > 10) ,'Rent'], na.rm = T)

```
The median rents of the LEED-rated buildings was found to be around 28.12, but those of LEED-only certified (which form the majority of the greeen buildings) was found to be 24. Clearly, this is our first confounding variable. Not *all* green buildings demand higher rent. In fact, LEED-only certified buildings have a lower median than non-green buildings!

#### 2. *net* rent builings and *green_rating*

It is quite important not to compare rents amongst those buildings which include utilities and those who don't separately. Let's take a look at the relative distribution of utilities-incuded buildings and vice-versa.

```{r net x-tabs, echo = F}
# Checking relationship between net with green_rating
greenRating_net <- xtabs(~green_rating
                          + net, data = greenBuildings)
ftable(greenRating_net) # print table 
```

Out of 685 green buildings, 646 buildings have net = 0. That is, utilities are included as part of their rent. This is a significant proprotion of green buildings (94 percent). Let's take a closer look at the medians.


```{r net summaries}
summary(greenBuildings[which(greenBuildings$net == 1 
                            & greenBuildings$green_rating == 0 
                            & greenBuildings$leasing_rate > 10) ,'Rent'],na.rm = T)
summary(greenBuildings[which(greenBuildings$net == 0 
                            & greenBuildings$green_rating == 0 
                            & greenBuildings$leasing_rate > 10) ,'Rent'],na.rm = T)
summary(greenBuildings[which(greenBuildings$net == 0 
                            & greenBuildings$green_rating == 1 
                            & greenBuildings$leasing_rate > 10) ,'Rent'],na.rm = T)
summary(greenBuildings[which(greenBuildings$net == 1 
                            & greenBuildings$green_rating == 1 
                            & greenBuildings$leasing_rate > 10) ,'Rent'],na.rm = T)
```

Amongst green buildings, rent amongst those including utilities is 28.2 while those without utilities included is 22.29. This clearly shows that we cannot judge median rents of green buildings without first accounting for the fact that 94 percent of them include utilities (even though a similar number of non-green buildings also have net = 0).

#### 2. Relationship Classes *A* and *B* builings and *green_rating*

People tend to willing to pay more rent for buildings with higher quality. Hence it goes without saying that Class A buildings will demand more rent than a similar Class B building. Looking at their distribution:

```{r class A and B x-tabs, echo = F}
# Checking relationship between EnergyStar and LEED ratings with green_rating
greenRating_classAB <- xtabs(~green_rating
                              + class_a 
                              + class_b, data = greenBuildings)
ftable(greenRating_classAB) # print table 

EnergyStar_LEED_classAB <- xtabs(~Energystar
                                  + LEED
                                  + class_a 
                                  + class_b, data = greenBuildings)
ftable(EnergyStar_LEED_classAB) # print table 
```

546 out of 685 green buildings are class A buildings. This is a a lot higher than the porportion of Class A buildings (81 percent) amongst the rest which stands at 2611 (36.3 percent). Let's take a closer look at the medians.

```{r class A and B x-tabs summaries}
summary(greenBuildings[which(greenBuildings$class_a == 1 
                            & greenBuildings$green_rating == 0 
                            & greenBuildings$leasing_rate > 10) ,'Rent'], na.rm = T)
summary(greenBuildings[which(greenBuildings$class_a == 0 
                            & greenBuildings$green_rating == 0 
                            & greenBuildings$leasing_rate > 10) ,'Rent'], na.rm = T)
summary(greenBuildings[which(greenBuildings$class_a == 0 
                            & greenBuildings$green_rating == 1 
                            & greenBuildings$leasing_rate > 10) ,'Rent'], na.rm = T)
summary(greenBuildings[which(greenBuildings$class_a == 1 
                            & greenBuildings$green_rating == 1 
                            & greenBuildings$leasing_rate > 10) ,'Rent'], na.rm = T)

summary(greenBuildings[which(greenBuildings$class_b == 1 
                            & greenBuildings$green_rating == 0 
                            & greenBuildings$leasing_rate > 10) ,'Rent'], na.rm = T)
summary(greenBuildings[which(greenBuildings$class_b == 0 
                            & greenBuildings$green_rating == 0 
                            & greenBuildings$leasing_rate > 10) ,'Rent'], na.rm = T)
summary(greenBuildings[which(greenBuildings$class_b == 0 
                            & greenBuildings$green_rating == 1 
                            & greenBuildings$leasing_rate > 10) ,'Rent'], na.rm = T)
summary(greenBuildings[which(greenBuildings$class_b == 1 
                            & greenBuildings$green_rating == 1 
                            & greenBuildings$leasing_rate > 10) ,'Rent'], na.rm = T)

```

Amongst green buildings, rent amongst those which are Class A buildings is 28.44 while the same amongst Class B or C is 25.68. This clearly shows that we cannot judge median rents of green buildings without first accounting for the fact that 81 percent of them class A buildings, and this could be the only reason that their rent is higher.

#### Conclusion

We conclude by having idenified 4 variables that have to be controlled for before comparing green and non-green buildings: EnergyStar, LEED, net and class_a. Each of these 4 variables could by itself raise or lower rent by almost 5 dollars per square foot.

## Bootstrapping

```{r,echo=F,include=F}
library(mosaic)
library(fImport)
library(foreach)
```

```{r, echo=F}
n_days=20

mystocks = c("SPY", "TLT", "LQD", "EEM", "VNQ")
myprices = yahooSeries(mystocks, from='2005-01-01', to='2016-07-31')

YahooPricesToReturns = function(series){
  mycols = grep('Adj.Close', colnames(series)) # string search for adjusted Close
  closingprice = series[,mycols] # exracting columns with a match for search string
  N = nrow(closingprice) # number of rows
  
  percentreturn = as.data.frame(closingprice[2:N,]) / as.data.frame(closingprice[1:(N-1),]) - 1 
  # (X(t+1)-X(t))/X(t) => X(t+1)/X(t) -1
  mynames = strsplit(colnames(percentreturn), '.', fixed=TRUE)
  mynames = lapply(mynames, function(x) return(paste0(x[1], ".PctReturn")))
  colnames(percentreturn) = mynames
  as.matrix(na.omit(percentreturn)) # na.omit removes incomplete cases from the data
}
myreturns = YahooPricesToReturns(myprices)
pairs(myreturns)
return.today = resample(myreturns, 1, orig.ids=FALSE)
```

The correlation graph between stocks gives us an overview of how the stocks are related:

* A safer stock would be the one which does not vary much due to changes in other stocks and has positive returns.  In this case, LQD is one such stock with close to zero correlation with all other stocks except TLT. LQD has a negative correlation of 0.42 with TLT, which is again very less.

* Also, SPY, EEM and VNQ are positively correlated with each other. This makes them the riskier stocks  since they are sensitive to variations in other stocks. The Exact order of riskiness can e determined buu the means and standard deviations of individual stocks

* TLT is negatively correlated with SPY, EEM and VNQ but it is safer than these 3 stocks since the correlation is weaker. The correlation plots for TLT are loose and spread out.

```{r, echo=F, fig.width=7, fig.height=11}

risk_5=NULL
mean_5=NULL
sd_5=NULL
par(mfrow=c(3,2))
set.seed(1)
for (j in 1:ncol(myreturns)){
  # Now simulate many different possible trading years!
  sim1 = foreach(i=1:5000, .combine='rbind') %do% {
    totalwealth = 100000
    weights = 1
    holdings = weights * totalwealth
    wealthtracker = rep(0, n_days) # Set up a placeholder to track total wealth
    for(today in 1:n_days) {
      return.today = resample(myreturns[,j], 1, orig.ids=FALSE)
      holdings = holdings + holdings*return.today
      totalwealth = sum(holdings)
      wealthtracker[today] = totalwealth
    }
    wealthtracker
  }
  
  hist(sim1[,n_days]- 100000,25,main=paste('Histogram for ',colnames(myreturns)[j],sep=" "),xlim=c(-50000,50000),xlab = "Returns", breaks = 25)
  
  # Calculate 5% value at risk
  risk_5[j] = quantile(sim1[,n_days], 0.05) - 100000
  mean_5[j] = mean(sim1[,n_days])
  sd_5[j] = sd(sim1[,n_days])
}
```

 

The individual 5% Value at Risk for the 5 stocks in the order of SPY, TLT, LQD, EEM and VNQ are:
`r risk_5`

The individual means for the 5 stocks in the order of SPY, TLT, LQD, EEM and VNQ are:
`r mean_5`

The individual standard deviations for the 5 stocks in the order of SPY, TLT, LQD, EEM and VNQ are:
`r sd_5`

#### Even split: 20% of the assets in each of the five ETFs above.

```{r,echo=F,include=F}
dev.off()
```

```{r,echo=F,fig.width=7,fig.height=5}
## A ## 
# Update the value of your holdings
set.seed(1)
# Now simulate many different possible trading years!
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  totalwealth = 100000
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  holdings = weights * totalwealth
  wealthtracker = rep(0, n_days) # Set up a placeholder to track total wealth
  for(today in 1:n_days) {
    weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
    return.today = resample(myreturns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    totalwealth = sum(holdings)
    wealthtracker[today] = totalwealth
  }
  wealthtracker
}

hist(sim1[,n_days]- 100000,25,xlim= c(-40000,40000), main='Histogram for Even Split',xlab = "Returns")
```

The 5% Value at Risk for Even Split = `r (quantile(sim1[,n_days], 0.05) - 100000)`

#### Safe Investment

```{r,echo=F,fig.width=7,fig.height=5}

# Now simulate many different possible trading years!
set.seed(1)
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  totalwealth = 100000
  weights = c(0.1, 0.3, 0.6)
  wealthtracker = rep(0, n_days) # Set up a placeholder to track total wealth
  for(today in 1:n_days) {
    weights = c(0.1, 0.3, 0.6)
    holdings = weights * totalwealth
    return.today = resample(myreturns[,c(1,2,3)], 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    totalwealth = sum(holdings)
    wealthtracker[today] = totalwealth
  }
  wealthtracker
}

hist(sim1[,n_days]- 100000,25,xlim=c(-40000,40000),main='Histogram for Safe Stocks',xlab = "Returns")
```

We have established that the Assets : SPY, TLT and LQD are the safer stocks by looking at their individual histograms. The proportion that we have taken for the investment is `r weights` , respectively.

Using this proportion, the 5% value at risk comes out to be `r (quantile(sim1[,n_days], 0.05) - 100000)`

#### Aggressive Investment

```{r,echo=F,fig.width=7,fig.height=5}
# Now simulate many different possible trading years!
set.seed(1)

sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  totalwealth = 100000
  weights = c(0.5,0.5)
  wealthtracker = rep(0, n_days) # Set up a placeholder to track total wealth
  for(today in 1:n_days) {
    weights = c(0.5,0.5)
    holdings = weights * totalwealth
    return.today = resample(myreturns[,c(4,5)], 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    totalwealth = sum(holdings)
    wealthtracker[today] = totalwealth
  }
  wealthtracker
}

hist(sim1[,n_days]- 100000,25,xlim= c(-40000,40000),main='Histogram for Risky Slots',xlab = "Returns")
```

### Conclusion

We have established that the Assets : EEM and VNQ are the riskier stocks. The proportion that we have taken for the investment is `r weights`, respectively.

Using this proportion, the 5% value at risk comes out to be `r (quantile(sim1[,n_days], 0.05) - 100000)`

## Market segmentation
   
#### Inital Set-up and Loading the Data: 

```{r} 
library(flexclust)
library(ggplot2)
library(reshape2)
library(corrplot)
library(corrgram)

social_mkt = read.csv("files/social_marketing.csv")
```

Initial analysis of data suggested that 'spam' and 'adult' just noise and don't add any value to the analysis. 

```{r}
sm_wo_junk = social_mkt[,-c(36,37)]
sm_wo_id = sm_wo_junk[,-1]
```

Since annotators used both *uncategorized* and *chatter* for tweets that didn't fit into any of the  categories, we decided to club both of these together.
'
```{r}
sm_wo_id$chatter = sm_wo_id$uncategorized + sm_wo_id$chatter
sm_wo_id = sm_wo_id[,-5]
```

In order to get started with cluster formation and analysis, we began by exploring the correlations.

```{r}
correlation_matrix = cor(sm_wo_id)
corrplot(correlation_matrix, type="upper", order="hclust")
sm_wo_id_scaled <- scale(sm_wo_id, center=TRUE, scale=TRUE)

# Creating pair-wise correlations ordered by max correlation
zdf <- as.data.frame(as.table(cor(sm_wo_id)))
zdf_2 <- subset(zdf, (abs(Freq) > 0.4 & Var1 != Var2))
zdf_2[order(zdf_2$Freq,decreasing = T), ]
```

Looking at the correlation plot, its seems that there are 5-7 combinations of correlated variables. People with majority of tweets in these categories can be clustered together. 

In order to get an optimal number of clusters, we implemented the *Elbow* method which gave us an optimum cluster number.

```{r}
set.seed(1)
# Compute and plot wss for k = 2 to k = 15
k.max <- 15 # Maximal number of clusters
data <- sm_wo_id_scaled
wss <- sapply(1:k.max, 
        function(k){kmeans(data, k, nstart=10 )$tot.withinss})
plot(1:k.max, wss,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")

```

The optimal number of clusters as per the algorithm is 5-7. Now, we evaluate different clustering methods to get the clusters.

#### Heirarchical Clustering

```{r}
##Heirarchical Clustering ## 
par(mfrow=c(2,2))
# Form a pairwise distance matrix using the dist function
distance_matrix = dist(sm_wo_id_scaled, method='euclidean')

# Now run hierarchical clustering
hier_p = hclust(distance_matrix, method='average')
cluster1 = cutree(hier_p, k=5)

# Plot the dendrogram
plot(cluster1, cex=0.8)

# Now run hierarchical clustering
hier_p = hclust(distance_matrix, method='centroid')
cluster1 = cutree(hier_p, k=5)

# Plot the dendrogram
plot(cluster1, cex=0.8)

# Now run hierarchical clustering
hier_p = hclust(distance_matrix, method='complete')
cluster1 = cutree(hier_p, k=10)

# Plot the dendrogram
plot(cluster1, cex=0.8)

# Now run hierarchical clustering
hier_p = hclust(distance_matrix, method='single')
cluster1 = cutree(hier_p, k=5)

# Plot the dendrogram
plot(cluster1, cex=0.8)
ind =which(cluster1 == 3)

sm_wo_id_node2 = sm_wo_id[ind,]

```

As we can see in Hierarchical clustering, the depth of the tree is very huge. We tried to cut tree to 5 Clusters, but the results are hard to interpret and even after we cut the trees at lesser levels, we couldn't derive meaning from the different hierarchies.  


#### PCA

```{r}
## PCA ## 
Z = sm_wo_id
Z_normalized = scale(Z, scale=T, center=T)
pc1 = prcomp(as.matrix(Z), scale.=TRUE)
plot(pc1)

loadings = pc1$rotation
scores = pc1$x
qplot(scores[,1], scores[,2], xlab='Component 1', ylab='Component 2')


o1 = order(loadings[,1])
colnames(Z)[head(o1,25)]
colnames(Z)[tail(o1,25)]
```

The Principal Component analysis gave us almost 6 of significant components. Substantial information could not be extracted from 4-6 components to infer the cluster composition, or to understand why certain set of groups scored high on one or more of the PCs.

#### K-means

```{r}
## try kmeans 
library(factoextra)
library(cluster)
library(NbClust)

sm_wo_id_scaled <- scale(sm_wo_id, center=TRUE, scale=TRUE) 
```

We used K-means to get clusters varying between 4-7 and understand cluster composition.


```{r}

# K-means clustering with 4 
set.seed(1)
km.res <- kmeans(sm_wo_id_scaled, 4, nstart = 25)
# k-means group number of each observation
km.res$cluster

# Visualize k-means clusters
fviz_cluster(km.res, data = sm_wo_id_scaled, geom = "point",
             stand = FALSE, frame.type = "norm")

clusters_pars = km.res$centers
transposed = t(clusters_pars)
cluster_1 = transposed[which(abs(transposed[,1])>=0.5),1]
cluster_2 = transposed[which(abs(transposed[,2])>=0.5),2]
cluster_3 = transposed[which(abs(transposed[,3])>=0.5),3]
cluster_1
cluster_2
cluster_3
```


**Trying with 5 clusters:**


```{r}
# K-means clustering with 5
set.seed(1)
km.res <- kmeans(sm_wo_id_scaled, 5, nstart = 25)
# k-means group number of each observation
km.res$cluster


# Visualize k-means clusters
fviz_cluster(km.res, data = sm_wo_id_scaled, geom = "point",
             stand = FALSE, frame.type = "norm")


clusters_pars = km.res$centers
transposed = t(clusters_pars)
cluster_1 = transposed[which(abs(transposed[,1])>=0.5),1]
cluster_2 = transposed[which(abs(transposed[,2])>=0.5),2]
cluster_3 = transposed[which(abs(transposed[,3])>=0.5),3]
cluster_4 = transposed[which(abs(transposed[,4])>=0.5),4]
cluster_5 = transposed[which(abs(transposed[,5])>=0.5),5]

cluster_1 
cluster_2 
cluster_3 
cluster_4 
cluster_5
```

The cluster composition we got from five clusters makes sense intuitively and is interpretable. In order to visualize these clusters and understand their prominent characteristics, we used word cloud.  

```{r}
# A word cloud
par(mfrow=c(2,2))
library(wordcloud)
for (i in 2:5) {
wordcloud(colnames(sm_wo_id_scaled), km.res$centers[i,], min.freq=0, max.words=100, scale=c(5,.5))
}
```


